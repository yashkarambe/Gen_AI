{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sr-J90pYl84",
        "outputId": "210e3572-9817-4627-8297-f121be2e930b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0; python_version < \"3.10\" in /home/yash/.local/lib/python3.8/site-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 2.1 MB/s eta 0:00:011\n",
            "\u001b[?25hCollecting pdfminer.six==20231228\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 803 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /home/yash/.local/lib/python3.8/site-packages (from pdfplumber) (9.3.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/yash/.local/lib/python3.8/site-packages (from pdfminer.six==20231228->pdfplumber) (3.1.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /home/yash/.local/lib/python3.8/site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /home/yash/.local/lib/python3.8/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /home/yash/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n",
            "Installing collected packages: pdfminer.six, pypdfium2, pdfplumber\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20221105\n",
            "    Uninstalling pdfminer.six-20221105:\n",
            "      Successfully uninstalled pdfminer.six-20221105\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a8RPrbOYkEu",
        "outputId": "fdb96e33-7421-49c1-e8c9-df4f3dd99abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ATTENTION-BASED SPEECH RECOGNITION USING GAZE INFORMATIO N\n",
            "Osamu Segawa\n",
            "Chubu Electric Power Co., Inc.\n",
            "Segawa.Osamu@chuden.co.jpTomoki Hayashi, Kazuya Takeda\n",
            "Graduate School of Informatics\n",
            "Nagoya University\n",
            "hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp\n",
            "ABSTRACT\n",
            "We assume that there is a correlation between an utterance\n",
            "and a corresponding gaze object, and propose a new paradigm\n",
            "of multi-modal end-to-end speech recognition using multi-\n",
            "modal information, namely, utterances and corresponding\n",
            "gaze points. In our method, the system extracts acoustic\n",
            "features and corresponding images around gaze points, and\n",
            "inputs the information into the proposed attention-based m ul-\n",
            "tiple encoder-decoder networks. This makes it possible to\n",
            "integrate the two different modalities, and the performanc e\n",
            "of speech recognition is improved. To evaluate the proposed\n",
            "method, we prepared a simulation task of power-line control\n",
            "operations, and built a corpus that contains utterances and\n",
            "corresponding gaze points in the operations. We conducted\n",
            "an experimental evaluation using this corpus, and the resul ts\n",
            "showed the reduction in the CER, suggesting the effective-\n",
            "ness of the proposed method in which acoustic features and\n",
            "gaze information are integrated.\n",
            "Index Terms —end-to-end speech recognition, attention,\n",
            "multi-modal, gaze-point\n",
            "1. INTRODUCTION\n",
            "In this paper, we propose a new paradigm of multi-modal\n",
            "end-to-end speech recognition using multi-modal informa-\n",
            "tion, namely, utterances and corresponding gaze points. In\n",
            "some task-oriented operations, gaze points and linguistic\n",
            "information are closely related to each other. Therefore, a\n",
            "system should be able to estimate the mutual synchronizatio n\n",
            "relationship and use the information for complementation\n",
            "and prediction. In our method, the system extracts acoustic\n",
            "features and corresponding images around gaze points, and\n",
            "uses the information as attentions [1] in encoder-decoder\n",
            "networks. To realize this concept, we propose an attention-\n",
            "based multiple encoder-decoder architecture that estimat es\n",
            "character sequences directly using both acoustic features and\n",
            "image sequences. Applying the attention mechanism to both\n",
            "a sequence of acoustic features and a sequence of image fea-\n",
            "tures, the system can compensate for differences in the time\n",
            "resolution. Moreover, it is expected to automatically lear n\n",
            "the difference between the occurrences of both types of info r-mation. As a result, owing to the use of the feature in which\n",
            "two different modalities are integrated, the improvement o f\n",
            "recognition performance is expected. To evaluate the pro-\n",
            "posed method, we prepared the simulation task: power-line\n",
            "control operations, and built a corpus that contains uttera nces\n",
            "and corresponding gaze points in the operations. Utilizing\n",
            "this corpus, we evaluated the effectiveness of using gaze\n",
            "information to improve recognition performance.\n",
            "2. RELATED WORKS\n",
            "Owing to advances in sensing technologies, various kinds\n",
            "of signal, such as image, speech, biometric-signals and gaz e\n",
            "points, can now be acquired easily at the same time. With\n",
            "this background, a large number of studies using multi-moda l\n",
            "information have been carried out to improve recognition\n",
            "performance.\n",
            "2.1. Using image features\n",
            "Image data is a typical example of a multi-modal signal. Sev-\n",
            "eral researchers, dealing with multi-modal speech recogni -\n",
            "tion, proposed the use of both lip images and speech sig-\n",
            "nals [2, 3, 4] to improve speech recognition performance in\n",
            "noisy environments. Mroueh et al. [2] proposed a method\n",
            "in which two different neural networks use both acoustic fea -\n",
            "tures and lip images as input signals. Their method integrat es\n",
            "the posterior probability of each network to improve recogn i-\n",
            "tion performance. Noda et al. [3] proposed a method in which\n",
            "the system estimates a clean acoustic feature from a noisy\n",
            "acoustic feature using denoising autoencoder (DAE). Their\n",
            "method calculates the phoneme posterior probability using a\n",
            "CNN, which is trained as a phoneme classiﬁcation model us-\n",
            "ing lip images. They built a speech recognition system based\n",
            "on GMM-HMM using a clean acoustic feature and the poste-\n",
            "rior probability to improve the recognition performance in a\n",
            "noisy environment. Petridis et al. [4] proposed an end-to-e nd\n",
            "model in which speech signals and lip images are processed\n",
            "in a single network to estimate the word class directly.\n",
            "These studies demonstrated improved recognition perfor-\n",
            "mance using image information. However, the utilization of\n",
            "images other than lip images has not been considered.465 978-1-7281-0306-8/19/$31.00 ©2019 IEEE ASRU 2019\n",
            "2.2. Using gaze points\n",
            "Another approach using multi-modal information is to use\n",
            "“gaze points” acquired by an eye-tracking device. For ex-\n",
            "ample, Nguyen et al. [5] proposed an assistant system for\n",
            "describing lecture notes that generates annotations autom ati-\n",
            "cally using the user’s gaze point and extracts important sen -\n",
            "tences. Moreover, Vasudevan et al. [6] proposed an end-to-\n",
            "end deep learning method in which the system estimates the\n",
            "user’s attention area (bounding box) corresponding to the u t-\n",
            "terances using several pieces of information: texts of utte r-\n",
            "ances, gaze points, depth images, and body movement. The\n",
            "aim of their study is to improve the performance of object de-\n",
            "tection in video images using both gaze points and linguisti c\n",
            "information. Furthermore, texts of utterances are the give n\n",
            "information.\n",
            "In contrast, there is another approach using both gaze\n",
            "points and speech as multi-modal information to improve\n",
            "recognition performance. For example, Rasmussen et al. [7]\n",
            "proposed a method for tracking reading process using a com-\n",
            "bination of gaze points and speech recognition. Cooke [8]\n",
            "proposed a probabilistic model that combine eye movement\n",
            "and speech recognition in a small simulation task. However,\n",
            "these approaches are based on HMM-based framework.\n",
            "3. SPEECH RECOGNITION BASED ON ATTENTION\n",
            "Recently, end-to-end speech recognition algorithms have\n",
            "been actively developed. In the early stage, the end-to-end\n",
            "approach was a method based on connectionist temporal clas-\n",
            "siﬁcation (CTC) [9, 10]. However, once the effectiveness of\n",
            "the attention-based method [1, 11] was proven, a number of\n",
            "further studies were carried out.\n",
            "In this approach, an encoder-decoder network architecture\n",
            "[12] is used to perform a direct mapping from a sequence of\n",
            "input features into a sequence of texts. The encoder network\n",
            "converts the sequence of input features to that of discrimin a-\n",
            "tive hidden states, and the decoder network uses the attenti on\n",
            "mechanism to obtain the alignment between each element of\n",
            "the output sequence and the encoder hidden states. Then it\n",
            "estimates the output symbol using weighted averaged hidden\n",
            "states, which is based on the alignment, as the inputs of the\n",
            "decoder network. In contrast to the CTC-based approach, the\n",
            "attention-based method does not require any conditional in de-\n",
            "pendence assumptions including the Markov assumption, lan -\n",
            "guage models, and complex decoding. However, a non-causal\n",
            "alignment problem is caused by a too ﬂexible alignment of\n",
            "the attention mechanism [13]. To address this issue, the stu dy\n",
            "[13] combines the objective function of the attention-base d\n",
            "model with that of CTC to constrain ﬂexible alignments of\n",
            "the attention mechanism.\n",
            "In another study [14], multi-head attention (MHA) is used\n",
            "to obtain more suitable alignments. In MHA, multiple atten-\n",
            "tions are calculated, then integrated into a single attenti on.\n",
            "Fig. 1 .Overview of attention-based multiple encoder-decoder\n",
            "architecture.\n",
            "The use of MHA enables the model to jointly focus on in-\n",
            "formation from different representation subspaces at diff erent\n",
            "positions [15], leading to the improvement of the recogniti on\n",
            "performance. Although these studies demonstrated the im-\n",
            "provement of the recognition performance using a new net-\n",
            "work architecture, they did not consider the use of multi-\n",
            "modal signals.\n",
            "4. ATTENTION-BASED MULTIPLE\n",
            "ENCODER-DECODER\n",
            "4.1. Overview of proposed method\n",
            "An overview of the proposed method is shown in Fig.1. In our\n",
            "method, two encoder networks are employed: one is assigned\n",
            "to a sequence of acoustic features and the other is assigned t o\n",
            "images around gaze points, and an attention mechanism is ap-\n",
            "plied to each sequence of hidden states. Then, character-wi se\n",
            "hidden features are calculated using the attention weight a nd\n",
            "the sequence of hidden states mentioned above. The acous-\n",
            "tic features are time-series features such as a mel-ﬁlterba nk,\n",
            "and the images around gaze points represent square regions,\n",
            "which are cropped from subjective images based on the co-\n",
            "ordinates of gaze points acquired by an eye-tracking device .\n",
            "Thus, a sequence of images around gaze points is an interre-\n",
            "lated time-series of human gaze regions.\n",
            "4.2. Formulation\n",
            "The attention-based method directly estimates a posterior ,\n",
            "p(C|X,Y), whereX={x1,x2,...,xT}represents a se-\n",
            "quence of speech features, Y={y1,y2,...,yN}represents\n",
            "a sequence of image features, and C={c1,c2,...,c L}\n",
            "represents a sequence of output characters. The posterior\n",
            "p(C|X,Y)is factorized with a probabilistic chain rule as\n",
            "follows:\n",
            "p(C|X,Y) =L/productdisplay\n",
            "l=1p(cl|c1:l−1,X,Y) (1)466\n",
            "wherec1:l−1represents a subsequence {c1,c2,... cl−1}. The\n",
            "posterior p(cl|c1:l−1,X,Y)is calculated as follows:\n",
            "H= AudioEncoder( X) (2)\n",
            "S= VideoEncoder( Y) (3)\n",
            "alt= LocationAttention( ql−1,ht,al−1) (4)\n",
            "bln= LocationAttention( ql−1,sn,bl−1) (5)\n",
            "¯hl= ΣT\n",
            "t=1altht (6)\n",
            "¯sl= ΣN\n",
            "n=1blnsn (7)\n",
            "gl=σ(Wg[¯h⊤\n",
            "l,¯s⊤\n",
            "l]⊤+bg) (8)\n",
            "rl=¯hl+gl⊙¯sl (9)\n",
            "p(cl|c1:l−1,X,Y) = Decoder( rl,ql−1,cl−1)(10)\n",
            "where equations (2) and (3) represent the audio encoder and\n",
            "the video encoder, respectively, and equation (10) represe nts\n",
            "the decoder network. ht,sn, andqlrepresent the hidden state\n",
            "vectors of the audio encoder, video encoder, and decoder net -\n",
            "works, respectively. HandSrepresents a sequence of hidden\n",
            "state vectors {h1,h2,...,hT}and{s1,s2,...,sN}, respec-\n",
            "tively.altandblnrepresents the attention weights for the se-\n",
            "quence of hidden state vectors in the audio encoder and video\n",
            "encoder network, respectively. ¯hland¯slrepresents hidden\n",
            "state vector for audio encoder and video encoder, which are\n",
            "weighted sum of the attention weights, respectively. glrep-\n",
            "resents the gate vector with a role of determining the priori ty\n",
            "of hidden vector of the video encoder, which is inspired by\n",
            "the ﬁne-gating introduced in Cold Fusion [16]. Finally, the\n",
            "input vector rlfor the decoder is calculated by the summation\n",
            "of hidden states ¯hlin the character-wise audio encoder and\n",
            "hidden states ¯slin the character-wise video encoder which is\n",
            "weighted by the gate vector gl.\n",
            "The audio encoder network is a neural network that con-\n",
            "verts an acoustic feature sequence X={x1,x2,...,xT}\n",
            "into a discriminative hidden vector sequence H={h1,h2,...\n",
            ",hT}, and the network is modeled by a CNN (VGG [17]) and\n",
            "bidirectional long short-term memory (BLSTM), where the\n",
            "VGG is a simpliﬁed network with four convolution layers and\n",
            "two pooling layers.\n",
            "AudioEncoder( X) = BLSTM(VGG( X)) (11)\n",
            "In the case of speech recognition, the sequence length signi f-\n",
            "icantly varies, therefore, the input sequence is shortened to a\n",
            "quarter of its original length using max-pooling in the VGG.\n",
            "The video encoder network converts an input image se-\n",
            "quenceY={y1,y2,...,yN}into a discriminative hidden\n",
            "vector sequence S={s1,s2,...,sN}, and it is modeled by\n",
            "AlexNet [18] and a recurrent neural network (BLSTM).\n",
            "VideoEncoder( Y) = BLSTM(AlexNet( Y)) (12)\n",
            "Because of insufﬁcient training data, AlexNet was retraine d\n",
            "using the ImageNet [19], one of a large image dataset.The attention weight altrepresents the alignment between\n",
            "an element clin the output sequence and the hidden state vec-\n",
            "torhtin the audio encoder. LocationAttention( ·)represents\n",
            "location-based attention [1] and is calculated as follows:\n",
            "Fl=K∗al−1 (13)\n",
            "elt=gTtanh(Wqql+Whht+Wfflt+b)(14)\n",
            "al= Softmax( el) (15)\n",
            "whereFlrepresents the vector sequence {fl1,fl2,...,flT},\n",
            "Krepresents a trainable convolutional ﬁlter, and the attent ion\n",
            "weightblnrepresents the alignment between elements clin\n",
            "the output sequence and the hidden state vector sn. These are\n",
            "calculated in the same manner as for alt.\n",
            "The decoder network estimates the next character clfrom\n",
            "the previous character cl−1, the hidden state vector of itself\n",
            "ql−1, and the character-wise hidden state vector rl, similarly\n",
            "to the RNN language model (RNNLM) [20]. It is typically\n",
            "modeled using LSTM as follows:\n",
            "ql= LSTM( cl,ql−1,rl) (16)\n",
            "Decoder( ·) = Softmax( Wql+b) (17)\n",
            "whereWandbrepresent trainable matrix and vector param-\n",
            "eters, respectively.\n",
            "Finally, all of the above networks are optimized using\n",
            "back-propagation through time (BPTT) [21] to minimize the\n",
            "following objective function:\n",
            "L=−logp(C|X,Y)\n",
            "=−log/parenleftBiggL/summationdisplay\n",
            "l=1p(cl|c∗\n",
            "1:l−1,X,Y)/parenrightBigg\n",
            "(18)\n",
            "wherec∗\n",
            "1:l−1={c∗\n",
            "1,c∗\n",
            "2,...,c∗\n",
            "l−1}represents the ground truth\n",
            "of the previous characters.\n",
            "5. EXPERIMENT\n",
            "5.1. Building Corpus\n",
            "To evaluate the proposed method, we built a corpus that con-\n",
            "tains both utterances and corresponding images around gaze\n",
            "points in the operations. We used a glass type eye-tracking\n",
            "device, Tobii Glass21, and prepared the simulation task of\n",
            "power-line control operations. The images are cropped to 12 8\n",
            "×128 pixels from subjective images referring to the coordi-\n",
            "nates of gaze points. In Tobii Glass2, the sampling rate for\n",
            "recording the gaze points is 50 Hz, and the frame rate of sub-\n",
            "jective images (1920 ×1080 pixels, MP4) using the built-in\n",
            "camera is 25 fps. Therefore, we resampled the gaze points to\n",
            "25 Hz.\n",
            "In the following, we explain the simulation task of power-\n",
            "line operations. In this task, the subjects execute the sequ ence\n",
            "1https://www.tobiipro.com467\n",
            "Fig. 2 .Example of simulation panel and gaze point.\n",
            "Table 1 . Example of operation sequence (these utterances\n",
            "were originally in Japanese).\n",
            "(1) Declaration of the operation procedure\n",
            "“AtIDAKA, IDAKA-INOKOSHI power-line No. 1,\n",
            "turn on line-switch 782.”\n",
            "(2) Selection of the button corresponding to the operation\n",
            "“Select line-switch 782.” (push the button)\n",
            "(3) Conﬁrmation of the operation procedure\n",
            "“Line-switch 782 was selected correctly.”\n",
            "(4) Execution of the operation\n",
            "“Turn on the line-switch.” (push the button)\n",
            "(5) Conﬁrmation of the operation result\n",
            "“AtIDAKA, IDAKA-INOKOSHI power-line No. 1,\n",
            "line-switch 782 was turned on correctly.”\n",
            "(6) Conﬁrmation of the time\n",
            "“It’s 15:35.”\n",
            "of operations. In each operation, the subjects utter the ope r-\n",
            "ation content and conﬁrm the operation results. The simula-\n",
            "tion panel and an example of a gaze point are shown in Fig.2;\n",
            "the center of the circle represents the coordinates of the ga ze\n",
            "point. To build the corpus, the subjects executed the task in\n",
            "several sessions comprising the sequence of operations sho wn\n",
            "in Table 1 (including six utterances). The subjects were fou r\n",
            "males with no work experience related to the task. In each\n",
            "recording session, both the label of button and the power-li ne\n",
            "name were changed. Examples of image sequences corre-\n",
            "sponding to utterances are shown in Fig.3.\n",
            "5.2. Experimental Setup\n",
            "An overview of the collected corpus is shown in Table 2. Us-\n",
            "ing the corpus, we conducted an experiment based on leave-\n",
            "one-subject-out (LOSO), that is, the data of two of the four\n",
            "speakers were used for the training, the data of one of the re-\n",
            "Fig. 3 .Examples of image sequences corresponding to utter-\n",
            "ances.\n",
            "maining speakers were used for the validation, and the data o f\n",
            "the fourth speakers were used for evaluation. In this scheme ,\n",
            "the character error rate (CER) was calculated for each eval-\n",
            "uation speaker, and the average CER is the ﬁnal evaluation\n",
            "result.\n",
            "The experimental conditions are shown in Table 3. For\n",
            "the experiment, we implemented the proposed method on the\n",
            "basis of the hybrid CTC/Attention architecture [13] (ESPne t\n",
            "[22]). Because of the insufﬁcient training data, we used the\n",
            "Corpus of Spontaneous Japanese [23] (CSJ) for pretraining\n",
            "of the initial model. First, in this procedure, the model is\n",
            "pretrained, except for the video encoder, using the whole se t\n",
            "of CSJ data. Second, the whole parameter of model includ-\n",
            "ing the video encoder is retrained using the task corpus and\n",
            "the initial weights of both the audio encoder and the decoder .\n",
            "In this training, we used the initial weight trained by Ima-\n",
            "geNet [19] for the feature extraction module (CNN) of the\n",
            "video encoder. In addition, we used the Joint CTC Attention\n",
            "multi-task learning [13], which uses the CTC objective func -\n",
            "tion together to learn the audio encoder. For the evaluation468\n",
            "Table 2 . Overview of corpus.\n",
            "Speaker ID Num. of Sessions Num. of Utterances\n",
            "SPK01 23 138\n",
            "SPK02 20 120\n",
            "SPK03 20 120\n",
            "SPK04 20 120\n",
            "measure, the following CER was used:\n",
            "CER =S+D+I\n",
            "N(19)\n",
            "where S, D, I and N are the number of substitution errors,\n",
            "deletion errors, insertion errors, and the total number of r ec-\n",
            "ognized characters, respectively. To evaluate the effecti ve-\n",
            "ness of the proposed method, we compared the following two\n",
            "models (without the video encoder).\n",
            "1. The model trained by the whole set of CSJ data.\n",
            "2. The model retrained with the data of the evaluation task\n",
            "using the initial weight learned with all the CSJ data.\n",
            "5.3. Experimental Results\n",
            "The experimental results are shown in Table 4. First, the res ult\n",
            "for the initial model (CSJ pretrain) is very low, because of\n",
            "the effect of unknown words in the task. Second, the result\n",
            "for the model (+ﬁne-tuning) is improved signiﬁcantly. This\n",
            "improvement is mainly due to the reduction in the number of\n",
            "unknown words and acoustic adaptation.\n",
            "The CER for the model (+video encoder) is reduced from\n",
            "7.2 % to 6.9 %. Thus, the results suggest that our proposed\n",
            "method using gaze information is effective for improving\n",
            "recognition performance. In addition, the CER for each\n",
            "speaker in the LOSO experiment is shown in Table 5. The\n",
            "difference in performance may be due to individuality in bot h\n",
            "utterances and gaze points. We will analyze the issue with a\n",
            "larger corpus.\n",
            "6. CONCLUSION\n",
            "In this study, we assume that there is a correlation between\n",
            "an utterance and a corresponding gaze object, and proposed a\n",
            "new paradigm of multi-modal end-to-end speech recognition\n",
            "using multi-modal information, namely, acoustic features and\n",
            "gaze points. To evaluate the proposed method, we prepared a\n",
            "simulation task of power-line control operations, and buil t a\n",
            "corpus that contains utterances and corresponding gaze poi nts\n",
            "in the operations. We conducted an experimental evaluation\n",
            "using this corpus. The results show the reduction in the CER\n",
            "suggesting the effectiveness of the proposed method in whic hTable 3 . Experimental condition.\n",
            "# CSJ training data 445,068\n",
            "# utterances in task corpus 498\n",
            "# unique characters 3,260\n",
            "sampling rate 16,000 Hz\n",
            "window size 25 ms\n",
            "shift size 10 ms\n",
            "audio encoder type VGG-BLSTM\n",
            "# audio encoder BLSTM layers 4\n",
            "# audio encoder BLSTM units 2,048\n",
            "# audio encoder projection units 1,024\n",
            "video encoder type AlexNet-BLSTM\n",
            "# video encoder BLSTM layers 1\n",
            "# video encoder BLSTM units 2,048\n",
            "# video encoder projection units 1,024\n",
            "audio encoder attention type Location-based\n",
            "kernel size in audio encoder attention 100\n",
            "# ﬁlters in audio encoder attention 10\n",
            "video encoder attention type Location-based\n",
            "kernel size in video encoder attention 20\n",
            "# ﬁlters in video encoder attention 10\n",
            "decoder type LSTM\n",
            "# decoder layers 1\n",
            "# decoder units 1,024\n",
            "learning rate 1.0\n",
            "dropout 0.2\n",
            "gradient clipping norm 5\n",
            "batch size 20 (pretrain)\n",
            "8 (ﬁne-tuning)\n",
            "maximum epoch 15\n",
            "beam size 20\n",
            "MTL alpha 0.5\n",
            "CTC weight in decoding 0.3\n",
            "Table 4. Experimental results.\n",
            "Model S [%] D [%] I [%] CER [%]\n",
            "CSJ pretrain 23.6 2.5 7.0 33.0\n",
            "+ ﬁne-tuning 5.3 0.9 1.0 7.2\n",
            "+ video encoder 5.2 1.1 0.7 6.9\n",
            "Table 5. Experimental results (for each speaker).\n",
            "Model (Speaker) S [%] D [%] I [%] CER [%]\n",
            "CSJ pretrain\n",
            "(SPK01) 31.4 4.8 7.4 43.7\n",
            "(SPK02) 21.0 1.5 10.4 32.9\n",
            "(SPK03) 19.4 2.7 7.6 29.7\n",
            "(SPK04) 22.4 0.9 2.6 25.8\n",
            "+ ﬁne-tuning\n",
            "(SPK01) 9.5 2.2 1.7 13.3\n",
            "(SPK02) 0.6 0.0 0.2 0.8\n",
            "(SPK03) 5.2 1.1 1.1 7.4\n",
            "(SPK04) 6.0 0.2 0.9 7.1\n",
            "+ video encoder\n",
            "(SPK01) 9.5 3.2 1.0 13.7\n",
            "(SPK02) 0.8 0.1 0.2 1.1\n",
            "(SPK03) 4.4 0.7 0.9 6.1\n",
            "(SPK04) 6.0 0.2 0.6 6.7469\n",
            "acoustic features and gaze information are integrated. In f u-\n",
            "ture work, we will analyze the attention weights of image\n",
            "sequences around gaze points and evaluate the performance\n",
            "using a larger corpus.\n",
            "7. REFERENCES\n",
            "[1] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and\n",
            "Y . Bengio, “Attention-based models for speech recog-\n",
            "nition,” in Advances in neural information processing\n",
            "systems , 2015, pp. 577–585.\n",
            "[2] Y . Mroueh, E. Marcheret, and V . Goel, “Deep mul-\n",
            "timodal learning for audio-visual speech recognition,”\n",
            "inAcoustics, Speech and Signal Processing (ICASSP),\n",
            "2015 IEEE International Conference on . IEEE, 2015,\n",
            "pp. 2130–2134.\n",
            "[3] K. Noda, Y . Yamaguchi, K. Nakadai, H. G. Okuno, and\n",
            "T. Ogata, “Audio-visual speech recognition using deep\n",
            "learning,” Applied Intelligence , vol. 42, no. 4, pp. 722–\n",
            "737, 2015.\n",
            "[4] S. Petridis, T. Stafylakis, P. Ma, F. Cai, G. Tzimiropou-\n",
            "los, and M. Pantic, “End-to-end audiovisual speech\n",
            "recognition,” arXiv preprint arXiv:1802.06424 , 2018.\n",
            "[5] C. Nguyen and F. Liu, “Gaze-based notetaking for learn-\n",
            "ing from lecture videos,” in Proceedings of the 2016\n",
            "CHI Conference on Human Factors in Computing Sys-\n",
            "tems. ACM, 2016, pp. 2093–2097.\n",
            "[6] A. B. Vasudevan, D. Dai, and L. Van Gool, “Object re-\n",
            "ferring in videos with language and human gaze,” arXiv\n",
            "preprint arXiv:1801.01582 , 2018.\n",
            "[7] M. H. Rasmussen and Z-H.Tan, “Fusing eye-gaze and\n",
            "speech recognition for tracking in an automatic reading\n",
            "tutor - a step in the right direction?” in Proc. SLaTE ,\n",
            "2013, pp. 112–115.\n",
            "[8] N. J. Cooke, “Gaze-contingent automatic speech recog-\n",
            "nition,” Ph.D thesis, Univ. of Birmingham , 2006.\n",
            "[9] A. Graves, S. Fern´ andez, F. Gomez, and J. Schmidhuber,\n",
            "“Connectionist temporal classiﬁcation: labelling unseg-\n",
            "mented sequence data with recurrent neural networks,”\n",
            "inProceedings of the 23rd international conference on\n",
            "Machine learning . ACM, 2006, pp. 369–376.\n",
            "[10] A. Graves and N. Jaitly, “Towards end-to-end speech\n",
            "recognition with recurrent neural networks,” in Inter-\n",
            "national Conference on Machine Learning , 2014, pp.\n",
            "1764–1772.\n",
            "[11] J. Chorowski, D. Bahdanau, K. Cho, and Y . Ben-\n",
            "gio, “End-to-end continuous speech recognition us-\n",
            "ing attention-based recurrent nn: First results,” arXiv\n",
            "preprint arXiv:1412.1602 , 2014.[12] K. Cho, B. Van Merri¨ enboer, C. Gulcehre, D. Bahdanau,\n",
            "F. Bougares, H. Schwenk, and Y . Bengio, “Learn-\n",
            "ing phrase representations using rnn encoder-decoder\n",
            "for statistical machine translation,” arXiv preprint\n",
            "arXiv:1406.1078 , 2014.\n",
            "[13] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and\n",
            "T. Hayashi, “Hybrid ctc/attention architecture for end-\n",
            "to-end speech recognition,” IEEE Journal of Selected\n",
            "Topics in Signal Processing , vol. 11, no. 8, pp. 1240–\n",
            "1253, 2017.\n",
            "[14] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar,\n",
            "P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao,\n",
            "K. Gonina et al. , “State-of-the-art speech recogni-\n",
            "tion with sequence-to-sequence models,” arXiv preprint\n",
            "arXiv:1712.01769 , 2017.\n",
            "[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\n",
            "L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n",
            "“Attention is all you need,” in Advances in Neural Infor-\n",
            "mation Processing Systems , 2017, pp. 6000–6010.\n",
            "[16] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold\n",
            "fusion: Training seq2seq models together with language\n",
            "models,” in Proc. Interspeech 2018 , 2018, pp. 387–391.\n",
            "[17] K. Simonyan and A. Zisserman, “Very deep convo-\n",
            "lutional networks for large-scale image recognition,”\n",
            "arXiv preprint arXiv:1409.1556 , 2014.\n",
            "[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-\n",
            "genet classiﬁcation with deep convolutional neural net-\n",
            "works,” in Advances in neural information processing\n",
            "systems , 2012, pp. 1097–1105.\n",
            "[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\n",
            "L. Fei-Fei, “Imagenet: A large-scale hierarchical image\n",
            "database,” 2009.\n",
            "[20] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. ˇCernock` y, and\n",
            "S. Khudanpur, “Recurrent neural network based lan-\n",
            "guage model,” in Eleventh Annual Conference of the In-\n",
            "ternational Speech Communication Association , 2010.\n",
            "[21] P. J. Werbos, “Backpropagation through time: what it\n",
            "does and how to do it,” Proceedings of the IEEE , vol. 78,\n",
            "no. 10, pp. 1550–1560, 1990.\n",
            "[22] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishi-\n",
            "toba, Y . Unno, N. E. Y . Soplin, J. Heymann, M. Wiesner,\n",
            "N. Chen et al. , “Espnet: End-to-end speech processing\n",
            "toolkit,” arXiv preprint arXiv:1804.00015 , 2018.\n",
            "[23] K. Maekawa, “Corpus of spontaneous japanese: Its de-\n",
            "sign and evaluation,” in ISCA & IEEE Workshop on\n",
            "Spontaneous Speech Processing and Recognition , 2003.470\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def pdf_to_text(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = 'ATTENTION-BASED SPEECH RECOGNITION USING GAZE INFORMATION.pdf'\n",
        "extracted_text = pdf_to_text(pdf_file_path)\n",
        "print(extracted_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Transformer library "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDx3AuxNaHYV",
        "outputId": "fac1b8ad-bd2b-47a6-d012-446e8fc41cac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-08 16:38:40.712673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-08 16:38:42.123112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/home/yash/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b6791871a0d4a72aefd06d2f881c4ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a189e83e52c496fb988bc61290e360d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c7a1dbb20564ec8b2142563658b8ff0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c77bbdfef954c5eac644eb55b91dd45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cccc53f9bf384bcca3dedaf1ffe912a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: bidirectional long short-term memory\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def pdf_to_text(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "# Function to get an answer from the local model\n",
        "def get_answer(question, context):\n",
        "    # Load a question-answering model\n",
        "    qa_pipeline = pipeline(\"question-answering\")\n",
        "\n",
        "    # Get the answer\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    return result['answer']\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = 'ATTENTION-BASED SPEECH RECOGNITION USING GAZE INFORMATION.pdf'  # Change to your PDF file path\n",
        "extracted_text = pdf_to_text(pdf_file_path)\n",
        "\n",
        "user_input = \"What are BLSTM?\"  # Example question\n",
        "answer = get_answer(user_input, extracted_text)\n",
        "\n",
        "# print(f\"Extracted Text:\\n{extracted_text}\\n\")\n",
        "print(f\"Answer: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Kp8FSdaHVE",
        "outputId": "844e1c6d-de82-40af-c374-6e0e3c55e0b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: hidden\n",
            "state vectors\n"
          ]
        }
      ],
      "source": [
        "user_input = \"What are acoustic feature sequence?\"  # Example question\n",
        "answer = get_answer(user_input, extracted_text)\n",
        "\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZiDTwh_fiPB",
        "outputId": "33778298-3599-4b04-ccfc-dc30627d89b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /home/yash/.local/lib/python3.8/site-packages (0.0.228)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /home/yash/.local/lib/python3.8/site-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (0.0.20)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (1.23.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (0.5.13)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/yash/.local/lib/python3.8/site-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /home/yash/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/yash/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/yash/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yash/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yash/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/yash/.local/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eoE0DbyfyZv",
        "outputId": "be2a5772-c680-4c40-d669-b7cbedcc45fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 1.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.112\n",
            "  Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 9.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses-json<0.7,>=0.5.7 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (0.5.13)\n",
            "Requirement already satisfied, skipping upgrade: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (8.2.2)\n",
            "Collecting langchain<0.3.0,>=0.2.16\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 9.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2,>=1; python_version < \"3.12\" in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (1.23.4)\n",
            "Collecting langchain-core<0.3.0,>=0.2.39\n",
            "  Downloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 9.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML>=5.3 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied, skipping upgrade: SQLAlchemy<3,>=1.4 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (2.0.19)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp<4.0.0,>=3.8.3 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (3.8.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2 in /home/yash/.local/lib/python3.8/site-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-toolbelt<2.0.0,>=1.0.0 in /home/yash/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<3,>=1; python_full_version < \"3.12.4\" in /home/yash/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: httpx<1,>=0.23.0 in /home/yash/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14\n",
            "  Downloading orjson-3.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 189 kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: marshmallow<4.0.0,>=3.18.0 in /home/yash/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-inspect<1,>=0.4.0 in /home/yash/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\" in /home/yash/.local/lib/python3.8/site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (4.0.2)\n",
            "Collecting typing-extensions>=4.7\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied, skipping upgrade: packaging<25,>=23.2 in /home/yash/.local/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (23.2)\n",
            "Requirement already satisfied, skipping upgrade: greenlet!=0.4.17; platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))) in /home/yash/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
            "Requirement already satisfied, skipping upgrade: charset-normalizer<4.0,>=2.0 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/yash/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain-community) (2022.12.7)\n",
            "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain-community) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /home/yash/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain-community) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: anyio in /home/yash/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: httpcore==1.* in /home/yash/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: sniffio in /home/yash/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: mypy-extensions>=0.3.0 in /home/yash/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.39->langchain-community) (2.0)\n",
            "Requirement already satisfied, skipping upgrade: exceptiongroup; python_version < \"3.11\" in /home/yash/.local/lib/python3.8/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h11<0.15,>=0.13 in /home/yash/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
            "\u001b[31mERROR: tf-agents 0.17.0 has requirement typing-extensions<4.6.0,>=3.7.4.3, but you'll have typing-extensions 4.12.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.13.1 has requirement typing-extensions<4.6.0,>=3.6.6, but you'll have typing-extensions 4.12.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ipykernel 6.29.0 has requirement pyzmq>=24, but you'll have pyzmq 19.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: argilla 1.13.1 has requirement httpx<0.24,>=0.15, but you'll have httpx 0.27.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: orjson, langsmith, typing-extensions, jsonpatch, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.5.0\n",
            "    Uninstalling typing-extensions-4.5.0:\n",
            "      Successfully uninstalled typing-extensions-4.5.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.228\n",
            "    Uninstalling langchain-0.0.228:\n",
            "      Successfully uninstalled langchain-0.0.228\n",
            "Successfully installed jsonpatch-1.33 langchain-0.2.16 langchain-community-0.2.17 langchain-core-0.2.41 langchain-text-splitters-0.2.4 langsmith-0.1.132 orjson-3.10.7 typing-extensions-4.12.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41c5vQoHf89Q",
        "outputId": "b3c48416-0e59-4aac-bf7d-57f79d1fa237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[K     |████████████████████████████████| 245 kB 2.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: scipy in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: Pillow in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (9.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (4.39.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (0.24.6)\n",
            "Requirement already satisfied: scikit-learn in /home/yash/.local/lib/python3.8/site-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/yash/.local/lib/python3.8/site-packages (from scipy->sentence-transformers) (1.23.4)\n",
            "Requirement already satisfied: typing-extensions in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.0.1)\n",
            "Requirement already satisfied: sympy in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.14.3)\n",
            "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: filelock in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.101)\n",
            "Requirement already satisfied: jinja2 in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.91)\n",
            "Requirement already satisfied: networkx in /home/yash/.local/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: requests in /home/yash/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/yash/.local/lib/python3.8/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/yash/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yash/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.11.0->sentence-transformers) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.11.0->sentence-transformers) (45.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/yash/.local/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: lit in /home/yash/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.11.0->sentence-transformers) (16.0.2)\n",
            "Requirement already satisfied: cmake in /home/yash/.local/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.11.0->sentence-transformers) (3.26.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/yash/.local/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yash/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.38.0->sentence-transformers) (2.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/yash/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.38.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yash/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.38.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/yash/.local/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.38.0->sentence-transformers) (2022.12.7)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC8C_wJpgplH",
        "outputId": "3490a0fe-77f4-4ff4-f5e8-76bbe302c71e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 10.4 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.0 MB 10.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: packaging in /home/yash/.local/lib/python3.8/site-packages (from faiss-cpu) (23.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /home/yash/.local/lib/python3.8/site-packages (from faiss-cpu) (1.23.4)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Langchain Framework "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqVhNuUAaHKl",
        "outputId": "8c9153da-09b7-489a-c529-c2860fe32f83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1762/1896046947.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85f148064de1489a9c64f6b9bd3353a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1c189dd25db4731b910c823d3179cc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb060218b2c94804af5aeb4c1c889562",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "982cdc93790e4158877f02b87bf00eee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yash/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db55e78afec64d05b5f3650859400d93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40b35d7be8b944a6bffacbca0a391a83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f70712ceec74097bd1f3238a87ad765",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "863f811ce4e9408db48436925f307f8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a871ff648a1e4929a4ce8963410f67ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e98bd82a6c341dc924f97cd0c443eac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "232c2c1f080d42f9832ddb3eca5775c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ca0ca3543704c65a78726432baf4b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c80cd755a5646afbc84e009979ff5d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff4a041df0d941b1bee3dce383ad0743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98cb4c8180f84b8ebb3e9455b6510756",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "419ecf5539b6445daac4251a1ce07398",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1762/1896046947.py:40: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  llm=HuggingFacePipeline(pipeline=qa_pipeline),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: corresponding images around gaze points\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def pdf_to_text(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = 'ATTENTION-BASED SPEECH RECOGNITION USING GAZE INFORMATION.pdf'  # Change to your PDF file path\n",
        "extracted_text = pdf_to_text(pdf_file_path)\n",
        "\n",
        "# Load extracted text into a Langchain-compatible format\n",
        "documents = [Document(page_content=extracted_text)]  # Wrap in a Document object\n",
        "\n",
        "# Create an embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create a vector store from the documents\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Load a question-answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Create a function to get an answer\n",
        "def get_answer(question, context):\n",
        "    return qa_pipeline(question=question, context=context)\n",
        "\n",
        "# Create a RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=HuggingFacePipeline(pipeline=qa_pipeline),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever()\n",
        ")\n",
        "\n",
        "# Ask a question\n",
        "user_input = \"What are acoustic feature sequence?\"  # Example question\n",
        "context = extracted_text  # Use the extracted text as context\n",
        "\n",
        "# Get the answer\n",
        "answer = get_answer(user_input, context)\n",
        "\n",
        "print(f\"Answer: {answer['answer']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wLwkzUyhBKI",
        "outputId": "eb782562-9eac-44b6-c6f2-3a5d85ae88d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: recurrent neural network\n"
          ]
        }
      ],
      "source": [
        "# Ask a question\n",
        "user_input = \"What are BLSTM?\"  # Example question\n",
        "context = extracted_text  # Use the extracted text as context\n",
        "\n",
        "# Get the answer\n",
        "answer = get_answer(user_input, context)\n",
        "\n",
        "print(f\"Answer: {answer['answer']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
